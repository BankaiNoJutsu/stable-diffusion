{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This colab requires you to have model.ckpt on your google drive (or you can download it on the next step)"
   ],
   "metadata": {
    "collapsed": false,
    "id": "f20_rzsJc1zU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JtgQHPFgc1zZ",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Install requirements\n",
    "!git clone https://github.com/neonsecret/stable-diffusion.git\n",
    "%cd /content/stable-diffusion\n",
    "!pip install gradio albumentations diffusers opencv-python pudb invisible-watermark imageio imageio-ffmpeg pytorch-lightning omegaconf test-tube streamlit einops torch-fidelity transformers torchmetrics kornia \n",
    "# !pip install taming\n",
    "!pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "!pip install git+https://github.com/crowsonkb/k-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Install requirements step 2\n",
    "%cd /content/stable-diffusion\n",
    "!pip install -e .\n",
    "!pip install Pillow==8.4.0 taming-transformers-rom1504\n",
    "!pip install --upgrade pytorch-lightning"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ksvqjvync1zb",
    "cellView": "form"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Mount colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "RJeVH0KtdVBh",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Stopping RUNTIME! If won't reconnect automatically, please run again.\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "metadata": {
    "cellView": "form",
    "id": "bFiD-_qJuNNu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkgBVo5OEpqn",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@markdown # Load the stable-diffusion model\n",
    "\n",
    "#@markdown **Download the model if it isn't already in the 'models_path' folder**\n",
    "\n",
    "#@markdown To download the model, you need to have accepted the terms [HERE](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n",
    "#@markdown and have copied a token from [HERE](https://huggingface.co/settings/tokens)\n",
    "download_if_missing = True #@param {type:\"boolean\"}\n",
    "token = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Google Drive Path Variables**\n",
    "mount_google_drive = True #@param {type:\"boolean\"}\n",
    "force_remount = False\n",
    "\n",
    "%cd /content/\n",
    "import os\n",
    "mount_success = True\n",
    "if mount_google_drive:\n",
    "    from google.colab import drive\n",
    "    try:\n",
    "        drive_path = \"/content/drive\"\n",
    "        drive.mount(drive_path,force_remount=force_remount)\n",
    "        models_path_gdrive = \"/content/drive/MyDrive/\" #@param {type:\"string\"}\n",
    "        output_path_gdrive = \"/content/drive/MyDrive/outputs\" #@param {type:\"string\"}\n",
    "        models_path = models_path_gdrive\n",
    "        output_path = output_path_gdrive\n",
    "    except:\n",
    "        print(\"...error mounting drive or with drive path variables\")\n",
    "        print(\"...reverting to default path variables\")\n",
    "        mount_success = False\n",
    "        output_path = \"/content/outputs\"\n",
    "\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "if download_if_missing:\n",
    "    if not mount_success:\n",
    "        print(\"Downloading model to \" + models_path + \" due to gdrive mount error\")\n",
    "    if token == \"\":\n",
    "        print(\"No token provided. Assuming model is already in \" + models_path)\n",
    "    elif not os.path.exists(models_path + '/sd-v1-4.ckpt'):\n",
    "        !git lfs install --system --skip-repo\n",
    "        !mkdir sd-model\n",
    "        %cd /content/sd-model/\n",
    "        !git init\n",
    "        !git remote add -f origin \"https://USER:{token}@huggingface.co/CompVis/stable-diffusion-v-1-4-original\"\n",
    "        !git config core.sparsecheckout true\n",
    "        !echo \"sd-v1-4.ckpt\" > .git/info/sparse-checkout\n",
    "        !git pull origin main\n",
    "        !mv '/content/sd-model/sd-v1-4.ckpt' '{models_path}/'\n",
    "    else:\n",
    "        print(\"Model already downloaded, moving to next step\")\n",
    "\n",
    "print(f\"models_path: {models_path}\")\n",
    "print(f\"output_path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load the models\n",
    "%cd /content/stable-diffusion\n",
    "import torch\n",
    "import argparse\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from itertools import islice\n",
    "from random import randint\n",
    "import sys\n",
    "sys.path.append(\"optimizedSD/\")\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import logging as transformers_logging\n",
    "import mimetypes\n",
    "from ldm.util import instantiate_from_config\n",
    "from optimUtils import split_weighted_subprompts\n",
    "\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "mimetypes.init()\n",
    "mimetypes.add_type(\"application/javascript\", \".js\")\n",
    "\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "async def get_logs():\n",
    "    return \"\\n\".join([x for x in open(\"log.txt\", \"r\", encoding=\"utf8\").readlines()] +\n",
    "                     [y for y in open(\"tqdm.txt\", \"r\", encoding=\"utf8\").readlines()])\n",
    "\n",
    "\n",
    "async def get_nvidia_smi():\n",
    "    proc = await asyncio.create_subprocess_shell('nvidia-smi', stdout=asyncio.subprocess.PIPE)\n",
    "    stdout, stderr = await proc.communicate()\n",
    "    return str(stdout)\n",
    "\n",
    "def load_model_from_config(ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    return sd\n",
    "\n",
    "\n",
    "config = \"optimizedSD/v1-inference.yaml\"\n",
    "ckpt = '/content/drive/MyDrive/sd-v1-4.ckpt' #@param {type:\"string\"}\n",
    "sd = load_model_from_config(f\"{ckpt}\")\n",
    "li, lo = [], []\n",
    "for key, v_ in sd.items():\n",
    "    sp = key.split(\".\")\n",
    "    if (sp[0]) == \"model\":\n",
    "        if \"input_blocks\" in sp:\n",
    "            li.append(key)\n",
    "        elif \"middle_block\" in sp:\n",
    "            li.append(key)\n",
    "        elif \"time_embed\" in sp:\n",
    "            li.append(key)\n",
    "        else:\n",
    "            lo.append(key)\n",
    "for key in li:\n",
    "    sd[\"model1.\" + key[6:]] = sd.pop(key)\n",
    "for key in lo:\n",
    "    sd[\"model2.\" + key[6:]] = sd.pop(key)\n",
    "\n",
    "config = OmegaConf.load(f\"{config}\")\n",
    "\n",
    "model = instantiate_from_config(config.modelUNet)\n",
    "_, _ = model.load_state_dict(sd, strict=False)\n",
    "model.eval()\n",
    "\n",
    "modelCS = instantiate_from_config(config.modelCondStage)\n",
    "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
    "modelCS.eval()\n",
    "\n",
    "modelFS = instantiate_from_config(config.modelFirstStage)\n",
    "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
    "modelFS.eval()\n",
    "del sd"
   ],
   "metadata": {
    "id": "voKbhBMEe-yg",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Okay next run one of the three (img2img, txt2img, inpainting)"
   ],
   "metadata": {
    "id": "NyafrRNTF1Xe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f56aa42bb10>,\n",
       " 'http://127.0.0.1:7862/',\n",
       " 'https://26054.gradio.app')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "#@title Optimized img2img\n",
    "from einops import rearrange, repeat\n",
    "def generate(\n",
    "        image,\n",
    "        prompt,\n",
    "        strength,\n",
    "        ddim_steps,\n",
    "        n_iter,\n",
    "        batch_size,\n",
    "        Height,\n",
    "        Width,\n",
    "        scale,\n",
    "        ddim_eta,\n",
    "        unet_bs,\n",
    "        device,\n",
    "        seed,\n",
    "        outdir,\n",
    "        img_format,\n",
    "        turbo,\n",
    "        full_precision,\n",
    "        sampler,\n",
    "        speed_mp\n",
    "):\n",
    "    logging.info(f\"prompt: {prompt}, W: {Width}, H: {Height}\")\n",
    "\n",
    "    init_image = load_img(image, Height, Width).to(device)\n",
    "    model.unet_bs = unet_bs\n",
    "    model.turbo = turbo\n",
    "    model.cdevice = device\n",
    "    modelCS.cond_stage_model.device = device\n",
    "\n",
    "    try:\n",
    "        seed = int(seed)\n",
    "    except:\n",
    "        seed = randint(0, 1000000)\n",
    "\n",
    "    if device != \"cpu\" and not full_precision:\n",
    "        model.half()\n",
    "        modelCS.half()\n",
    "        modelFS.half()\n",
    "        init_image = init_image.half()\n",
    "\n",
    "    tic = time.time()\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = outdir\n",
    "    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt)))[:150]\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    modelFS.to(device)\n",
    "\n",
    "    init_image = repeat(init_image, \"1 ... -> b ...\", b=batch_size)\n",
    "    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n",
    "\n",
    "    if device != \"cpu\":\n",
    "        mem = torch.cuda.memory_allocated() / 1e6\n",
    "        modelFS.to(\"cpu\")\n",
    "        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
    "            time.sleep(1)\n",
    "\n",
    "    assert 0.0 <= strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n",
    "    t_enc = int(strength * ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    if not full_precision and device != \"cpu\":\n",
    "        precision_scope = autocast\n",
    "    else:\n",
    "        precision_scope = nullcontext\n",
    "\n",
    "    all_samples = []\n",
    "    seeds = \"\"\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
    "            for prompts in tqdm(data, desc=\"data\"):\n",
    "                with precision_scope(\"cuda\"):\n",
    "                    modelCS.to(device)\n",
    "                    uc = None\n",
    "                    if scale != 1.0:\n",
    "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "\n",
    "                    subprompts, weights = split_weighted_subprompts(prompts[0])\n",
    "                    if len(subprompts) > 1:\n",
    "                        c = torch.zeros_like(uc)\n",
    "                        totalWeight = sum(weights)\n",
    "                        # normalize each \"sub prompt\" and add it\n",
    "                        for i in range(len(subprompts)):\n",
    "                            weight = weights[i]\n",
    "                            # if not skip_normalize:\n",
    "                            weight = weight / totalWeight\n",
    "                            c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
    "                    else:\n",
    "                        c = modelCS.get_learned_conditioning(prompts)\n",
    "\n",
    "                    if device != \"cpu\":\n",
    "                        mem = torch.cuda.memory_allocated() / 1e6\n",
    "                        modelCS.to(\"cpu\")\n",
    "                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
    "                            time.sleep(1)\n",
    "\n",
    "                    # encode (scaled latent)\n",
    "                    z_enc = model.stochastic_encode(\n",
    "                        init_latent, torch.tensor([t_enc] * batch_size).to(device), seed, ddim_eta, ddim_steps\n",
    "                    )\n",
    "                    # decode it\n",
    "                    samples_ddim = model.sample(\n",
    "                        t_enc,\n",
    "                        c,\n",
    "                        z_enc,\n",
    "                        unconditional_guidance_scale=scale,\n",
    "                        unconditional_conditioning=uc,\n",
    "                        sampler=sampler,\n",
    "                        speed_mp=speed_mp,\n",
    "                        batch_size=batch_size\n",
    "                    )\n",
    "\n",
    "                    modelFS.to(device)\n",
    "                    print(\"saving images\")\n",
    "                    for i in range(batch_size):\n",
    "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
    "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        all_samples.append(x_sample.to(\"cpu\"))\n",
    "                        x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.{img_format}\")\n",
    "                        )\n",
    "                        seeds += str(seed) + \",\"\n",
    "                        seed += 1\n",
    "                        base_count += 1\n",
    "\n",
    "                    if device != \"cpu\":\n",
    "                        mem = torch.cuda.memory_allocated() / 1e6\n",
    "                        modelFS.to(\"cpu\")\n",
    "                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
    "                            time.sleep(1)\n",
    "\n",
    "                    del samples_ddim\n",
    "                    del x_sample\n",
    "                    del x_samples_ddim\n",
    "                    print(\"memory_final = \", torch.cuda.memory_allocated() / 1e6)\n",
    "\n",
    "    toc = time.time()\n",
    "\n",
    "    time_taken = (toc - tic) / 60.0\n",
    "    grid = torch.cat(all_samples, 0)\n",
    "    grid = make_grid(grid, nrow=n_iter)\n",
    "    grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n",
    "\n",
    "    txt = (\n",
    "            \"Samples finished in \"\n",
    "            + str(round(time_taken, 3))\n",
    "            + \" minutes and exported to \\n\"\n",
    "            + sample_path\n",
    "            + \"\\nSeeds used = \"\n",
    "            + seeds[:-1]\n",
    "    )\n",
    "    return Image.fromarray(grid.astype(np.uint8)), txt\n",
    "\n",
    "def load_img(image, h0, w0):\n",
    "    image = image.convert(\"RGB\")\n",
    "    w, h = image.size\n",
    "    print(f\"loaded input image of size ({w}, {h})\")\n",
    "    if h0 is not None and w0 is not None:\n",
    "        h, w = h0, w0\n",
    "\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
    "\n",
    "    print(f\"New image size ({w}, {h})\")\n",
    "    image = image.resize((w, h), resample=Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"# Stable diffusion img2img (neonsecret's adjustments)\")\n",
    "        gr.Markdown(\"### Press 'print logs' button to get the model output logs\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                outs1 = [gr.Image(label=\"Output Image\"), gr.Text(label=\"Generation results\")]\n",
    "                outs2 = [gr.Text(label=\"Logs\")]\n",
    "                outs3 = [gr.Text(label=\"nvidia-smi\")]\n",
    "                b1 = gr.Button(\"Generate!\")\n",
    "                b2 = gr.Button(\"Print logs\")\n",
    "                b3 = gr.Button(\"nvidia-smi\")\n",
    "            with gr.Column():\n",
    "                with gr.Box():\n",
    "                    b1.click(generate, inputs=[\n",
    "                        gr.Image(tool=\"editor\", type=\"pil\", label=\"Initial image\"),\n",
    "                        gr.Text(label=\"Your Prompt\"),\n",
    "                        gr.Slider(0, 1, value=0.75, label=\"Generated image strength\"),\n",
    "                        gr.Slider(1, 1000, value=50, label=\"Sampling Steps\"),\n",
    "                        gr.Slider(1, 100, step=1, label=\"Number of images\"),\n",
    "                        gr.Slider(1, 100, step=1, label=\"Batch size\"),\n",
    "                        gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n",
    "                        gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n",
    "                        gr.Slider(0, 50, value=7.5, step=0.1, label=\"Guidance scale\"),\n",
    "                        gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n",
    "                        gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n",
    "                        gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n",
    "                        gr.Text(label=\"Seed\"),\n",
    "                        gr.Text(value=output_path, label=\"Outputs path\"),\n",
    "                        gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n",
    "                        gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n",
    "                        gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n",
    "                        gr.Radio([\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"], value=\"plms\", label=\"Sampler\"),\n",
    "                        gr.Slider(1, 100, value=100, step=1,\n",
    "                                  label=\"%, VRAM usage limiter (100 means max speed)\"),\n",
    "                    ], outputs=outs1)\n",
    "                    b2.click(get_logs, inputs=[], outputs=outs2)\n",
    "                    b3.click(get_nvidia_smi, inputs=[], outputs=outs3)\n",
    "\n",
    "debug = False #@param {type:\"boolean\"}\n",
    "demo.launch(share=True, debug=debug)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0FgHkm_Ic1zb",
    "outputId": "0959b5dc-1885-4387-cfce-0b8b46e7071a",
    "cellView": "form"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "Running on public URL: https://18776.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://18776.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 601208\n",
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "data:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds used =  [601208]\n",
      "Data shape for PLMS sampling is [1, 4, 224, 224]\n",
      "Running PLMS Sampling with 1 timesteps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e5c175178442329947c6bf6a87ba55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PLMS Sampler:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "data: 100%|██████████| 1/1 [01:16<00:00, 76.17s/it]\n",
      "Sampling: 100%|██████████| 1/1 [01:16<00:00, 76.19s/it]\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 846492\n",
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "data:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds used =  [846492]\n",
      "Data shape for PLMS sampling is [1, 4, 144, 256]\n",
      "Running PLMS Sampling with 1 timesteps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c24ce73f35e4e158f442673f9e9a147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PLMS Sampler:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "data: 100%|██████████| 1/1 [00:40<00:00, 40.17s/it]\n",
      "Sampling: 100%|██████████| 1/1 [00:40<00:00, 40.19s/it]\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 22735\n",
      "Sampling:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "data:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "seeds used =  [22735]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34d70368dda94772aaa097c4f71218d4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "data: 100%|██████████| 1/1 [26:24<00:00, 1584.11s/it]\n",
      "Sampling: 100%|██████████| 1/1 [26:24<00:00, 1584.13s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f562a29a750>,\n",
       " 'http://127.0.0.1:7861/',\n",
       " 'https://18776.gradio.app')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#@title Optimized txt2img\n",
    "def generate(\n",
    "        prompt,\n",
    "        ddim_steps,\n",
    "        n_iter,\n",
    "        batch_size,\n",
    "        Height,\n",
    "        Width,\n",
    "        scale,\n",
    "        ddim_eta,\n",
    "        unet_bs,\n",
    "        device,\n",
    "        seed,\n",
    "        outdir,\n",
    "        img_format,\n",
    "        turbo,\n",
    "        full_precision,\n",
    "        sampler,\n",
    "        speed_mp\n",
    "):\n",
    "    logging.info(f\"prompt: {prompt}, W: {Width}, H: {Height}\")\n",
    "    C = 4\n",
    "    f = 8\n",
    "    start_code = None\n",
    "    model.to(device)\n",
    "    model.unet_bs = unet_bs\n",
    "    model.turbo = turbo\n",
    "    model.cdevice = device\n",
    "    modelCS.cond_stage_model.device = device\n",
    "\n",
    "    if seed == \"\":\n",
    "        seed = randint(0, 1000000)\n",
    "    seed = int(seed)\n",
    "    seed_everything(seed)\n",
    "\n",
    "    if device != \"cpu\" and not full_precision:\n",
    "        model.half()\n",
    "        modelFS.half()\n",
    "        modelCS.half()\n",
    "\n",
    "    tic = time.time()\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = outdir\n",
    "    sample_path = os.path.join(outpath, \"_\".join(re.split(\":| \", prompt.replace(\"/\", \"\"))))[:150]\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "\n",
    "    if device != \"cpu\" and not full_precision:\n",
    "        precision_scope = autocast\n",
    "    else:\n",
    "        precision_scope = nullcontext\n",
    "\n",
    "    seeds = \"\"\n",
    "    with torch.no_grad():\n",
    "        all_samples = list()\n",
    "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
    "            for prompts in tqdm(data, desc=\"data\"):\n",
    "                with precision_scope(\"cuda\"):\n",
    "                    modelCS.to(device)\n",
    "                    uc = None\n",
    "                    if scale != 1.0:\n",
    "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "\n",
    "                    subprompts, weights = split_weighted_subprompts(prompts[0])\n",
    "                    if len(subprompts) > 1:\n",
    "                        c = torch.zeros_like(uc)\n",
    "                        totalWeight = sum(weights)\n",
    "                        # normalize each \"sub prompt\" and add it\n",
    "                        for i in range(len(subprompts)):\n",
    "                            weight = weights[i]\n",
    "                            # if not skip_normalize:\n",
    "                            weight = weight / totalWeight\n",
    "                            c = torch.add(c, modelCS.get_learned_conditioning(subprompts[i]), alpha=weight)\n",
    "                    else:\n",
    "                        c = modelCS.get_learned_conditioning(prompts)\n",
    "\n",
    "                    shape = [batch_size, C, Height // f, Width // f]\n",
    "\n",
    "                    if device != \"cpu\":\n",
    "                        mem = torch.cuda.memory_allocated() / 1e6\n",
    "                        modelCS.to(\"cpu\")\n",
    "                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
    "                            time.sleep(1)\n",
    "                    samples_ddim = model.sample(\n",
    "                        S=ddim_steps,\n",
    "                        conditioning=c,\n",
    "                        seed=seed,\n",
    "                        shape=shape,\n",
    "                        verbose=False,\n",
    "                        unconditional_guidance_scale=scale,\n",
    "                        unconditional_conditioning=uc,\n",
    "                        eta=ddim_eta,\n",
    "                        x_T=start_code,\n",
    "                        sampler=sampler,\n",
    "                        speed_mp=speed_mp\n",
    "                    )\n",
    "\n",
    "                    modelFS.to(device)\n",
    "                    model.cpu()\n",
    "                    logging.info(\"saving images\")\n",
    "                    for i in range(batch_size):\n",
    "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
    "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        all_samples.append(x_sample.to(\"cpu\"))\n",
    "                        x_sample = 255.0 * rearrange(x_sample[0].cpu().numpy(), \"c h w -> h w c\")\n",
    "                        Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                            os.path.join(sample_path, \"seed_\" + str(seed) + \"_\" + f\"{base_count:05}.{img_format}\")\n",
    "                        )\n",
    "                        seeds += str(seed) + \",\"\n",
    "                        seed += 1\n",
    "                        base_count += 1\n",
    "\n",
    "                    if device != \"cpu\":\n",
    "                        mem = torch.cuda.memory_allocated() / 1e6\n",
    "                        modelFS.to(\"cpu\")\n",
    "                        while torch.cuda.memory_allocated() / 1e6 >= mem:\n",
    "                            time.sleep(1)\n",
    "\n",
    "                    del samples_ddim\n",
    "                    del x_sample\n",
    "                    del x_samples_ddim\n",
    "                    logging.info(str(\"memory_final = \" + str(torch.cuda.memory_allocated() / 1e6)))\n",
    "\n",
    "    toc = time.time()\n",
    "\n",
    "    time_taken = (toc - tic) / 60.0\n",
    "    grid = torch.cat(all_samples, 0)\n",
    "    grid = make_grid(grid, nrow=n_iter)\n",
    "    grid = 255.0 * rearrange(grid, \"c h w -> h w c\").cpu().numpy()\n",
    "    txt = (\n",
    "            \"Samples finished in \"\n",
    "            + str(round(time_taken, 3))\n",
    "            + \" minutes and exported to \"\n",
    "            + sample_path\n",
    "            + \"\\nSeeds used = \"\n",
    "            + seeds[:-1]\n",
    "    )\n",
    "    return Image.fromarray(grid.astype(np.uint8)), txt\n",
    "\n",
    "\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"# Stable diffusion txt2img (neonsecret's adjustments)\")\n",
    "        gr.Markdown(\"### Press 'print logs' button to get the model output logs\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                outs1 = [gr.Image(label=\"Output Image\"), gr.Text(label=\"Generation results\")]\n",
    "                outs2 = [gr.Text(label=\"Logs\")]\n",
    "                outs3 = [gr.Text(label=\"nvidia-smi\")]\n",
    "                b1 = gr.Button(\"Generate!\")\n",
    "                b2 = gr.Button(\"Print logs\")\n",
    "                b3 = gr.Button(\"nvidia-smi\")\n",
    "            with gr.Column():\n",
    "                with gr.Box():\n",
    "                    b1.click(generate, inputs=[\n",
    "                        gr.Text(label=\"Your Prompt\"),\n",
    "                        gr.Slider(1, 1000, value=50, label=\"Sampling Steps\"),\n",
    "                        gr.Slider(1, 100, step=1, label=\"Number of images\"),\n",
    "                        gr.Slider(1, 100, step=1, label=\"Batch size\"),\n",
    "                        gr.Slider(64, 4096, value=512, step=64, label=\"Height\"),\n",
    "                        gr.Slider(64, 4096, value=512, step=64, label=\"Width\"),\n",
    "                        gr.Slider(0, 50, value=7.5, step=0.1, label=\"Guidance scale\"),\n",
    "                        gr.Slider(0, 1, step=0.01, label=\"DDIM sampling ETA\"),\n",
    "                        gr.Slider(1, 2, value=1, step=1, label=\"U-Net batch size\"),\n",
    "                        gr.Radio([\"cuda\", \"cpu\"], value=\"cuda\", label=\"Device\"),\n",
    "                        gr.Text(label=\"Seed\"),\n",
    "                        gr.Text(value=output_path, label=\"Outputs path\"),\n",
    "                        gr.Radio([\"png\", \"jpg\"], value='png', label=\"Image format\"),\n",
    "                        gr.Checkbox(value=True, label=\"Turbo mode (better leave this on)\"),\n",
    "                        gr.Checkbox(label=\"Full precision mode (practically does nothing)\"),\n",
    "                        gr.Radio([\"ddim\", \"plms\", \"k_dpm_2_a\", \"k_dpm_2\", \"k_euler_a\", \"k_euler\", \"k_heun\", \"k_lms\"], value=\"plms\", label=\"Sampler\"),\n",
    "                        gr.Slider(1, 100, value=100, step=1,\n",
    "                                  label=\"%, VRAM usage limiter (100 means max speed)\"),\n",
    "                    ], outputs=outs1)\n",
    "                    b2.click(get_logs, inputs=[], outputs=outs2)\n",
    "                    b3.click(get_nvidia_smi, inputs=[], outputs=outs3)\n",
    "\n",
    "debug = False #@param {type:\"boolean\"}\n",
    "demo.launch(share=True, debug=debug)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gnYhjq2Gc1zc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "20e5c175178442329947c6bf6a87ba55",
      "d1df4403f9634329aefdb6257a8f3683",
      "7b5ba2c15f7c4004a53bc9ae5835ec24",
      "52a85079665a4c05b385044c23109cd5",
      "748256e85f5342e6917bd678b9dc53e5",
      "f8b6a393d2404e29a32878c68a8def6b",
      "32f5abc5b89c43c48fafe01c47077ee1",
      "9b84516fb80649da981b801bcca21f8f",
      "d41647bf0c524c1bb4d5f00cb7238aea",
      "cdcc4a3587d6435a85b61fbf01103905",
      "abc4102cfa69402087507cbe9a5e3598",
      "7c24ce73f35e4e158f442673f9e9a147",
      "1f87ff3cd3fd4687b230ad629910e038",
      "76b6650094364291979c056aa8f78f45",
      "c1fc09be24d1423abe0b2668f5ad5669",
      "7fd3f6c2ad464fa4959c80ee9d691d6c",
      "6e08a5a89d4146a39626859e1b642a84",
      "c37f57a26d644946b54341bfcb4043b0",
      "2f9edb3520a14e6185854c49595911d8",
      "b8ce73de71f34d66b15478f126a1569e",
      "55ebef693b0844cca45e5faca00c0d3d",
      "b1091d7edebe4d9895d7224786bdaa77",
      "34d70368dda94772aaa097c4f71218d4",
      "f71f9837ac2641019151c524f62723d5",
      "04f60bb8529a44a7bcecd82454fe3843",
      "a1bcca0fa79f4536936cf881067b289a",
      "f313fb133bc44ede8ca2ed61c88b70d1",
      "be8aec92962b4dcaac14244f95198570",
      "7134d32dc7094c48a984ab464f9e4bae",
      "ced495e8d49940ceb7ce900c229c7d04",
      "ac2538e758fa4b58930ae563cbb18db1",
      "63e2b2ab3b8c48e68dfeaec8b5f4128d",
      "020e0c9d87e54f74b81c1f62b3721802"
     ]
    },
    "outputId": "20b2be01-da26-43fb-da61-3eb25622c803",
    "cellView": "form"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "20e5c175178442329947c6bf6a87ba55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d1df4403f9634329aefdb6257a8f3683",
       "IPY_MODEL_7b5ba2c15f7c4004a53bc9ae5835ec24",
       "IPY_MODEL_52a85079665a4c05b385044c23109cd5"
      ],
      "layout": "IPY_MODEL_748256e85f5342e6917bd678b9dc53e5"
     }
    },
    "d1df4403f9634329aefdb6257a8f3683": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8b6a393d2404e29a32878c68a8def6b",
      "placeholder": "​",
      "style": "IPY_MODEL_32f5abc5b89c43c48fafe01c47077ee1",
      "value": "PLMS Sampler: 100%"
     }
    },
    "7b5ba2c15f7c4004a53bc9ae5835ec24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b84516fb80649da981b801bcca21f8f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d41647bf0c524c1bb4d5f00cb7238aea",
      "value": 1
     }
    },
    "52a85079665a4c05b385044c23109cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdcc4a3587d6435a85b61fbf01103905",
      "placeholder": "​",
      "style": "IPY_MODEL_abc4102cfa69402087507cbe9a5e3598",
      "value": " 1/1 [01:01&lt;00:00, 61.58s/it]"
     }
    },
    "748256e85f5342e6917bd678b9dc53e5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8b6a393d2404e29a32878c68a8def6b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32f5abc5b89c43c48fafe01c47077ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b84516fb80649da981b801bcca21f8f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d41647bf0c524c1bb4d5f00cb7238aea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cdcc4a3587d6435a85b61fbf01103905": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abc4102cfa69402087507cbe9a5e3598": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7c24ce73f35e4e158f442673f9e9a147": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1f87ff3cd3fd4687b230ad629910e038",
       "IPY_MODEL_76b6650094364291979c056aa8f78f45",
       "IPY_MODEL_c1fc09be24d1423abe0b2668f5ad5669"
      ],
      "layout": "IPY_MODEL_7fd3f6c2ad464fa4959c80ee9d691d6c"
     }
    },
    "1f87ff3cd3fd4687b230ad629910e038": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e08a5a89d4146a39626859e1b642a84",
      "placeholder": "​",
      "style": "IPY_MODEL_c37f57a26d644946b54341bfcb4043b0",
      "value": "PLMS Sampler: 100%"
     }
    },
    "76b6650094364291979c056aa8f78f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f9edb3520a14e6185854c49595911d8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b8ce73de71f34d66b15478f126a1569e",
      "value": 1
     }
    },
    "c1fc09be24d1423abe0b2668f5ad5669": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55ebef693b0844cca45e5faca00c0d3d",
      "placeholder": "​",
      "style": "IPY_MODEL_b1091d7edebe4d9895d7224786bdaa77",
      "value": " 1/1 [00:31&lt;00:00, 31.70s/it]"
     }
    },
    "7fd3f6c2ad464fa4959c80ee9d691d6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e08a5a89d4146a39626859e1b642a84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c37f57a26d644946b54341bfcb4043b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f9edb3520a14e6185854c49595911d8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8ce73de71f34d66b15478f126a1569e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55ebef693b0844cca45e5faca00c0d3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1091d7edebe4d9895d7224786bdaa77": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34d70368dda94772aaa097c4f71218d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f71f9837ac2641019151c524f62723d5",
       "IPY_MODEL_04f60bb8529a44a7bcecd82454fe3843",
       "IPY_MODEL_a1bcca0fa79f4536936cf881067b289a"
      ],
      "layout": "IPY_MODEL_f313fb133bc44ede8ca2ed61c88b70d1"
     }
    },
    "f71f9837ac2641019151c524f62723d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be8aec92962b4dcaac14244f95198570",
      "placeholder": "​",
      "style": "IPY_MODEL_7134d32dc7094c48a984ab464f9e4bae",
      "value": "100%"
     }
    },
    "04f60bb8529a44a7bcecd82454fe3843": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ced495e8d49940ceb7ce900c229c7d04",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac2538e758fa4b58930ae563cbb18db1",
      "value": 50
     }
    },
    "a1bcca0fa79f4536936cf881067b289a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63e2b2ab3b8c48e68dfeaec8b5f4128d",
      "placeholder": "​",
      "style": "IPY_MODEL_020e0c9d87e54f74b81c1f62b3721802",
      "value": " 50/50 [26:15&lt;00:00, 31.51s/it]"
     }
    },
    "f313fb133bc44ede8ca2ed61c88b70d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be8aec92962b4dcaac14244f95198570": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7134d32dc7094c48a984ab464f9e4bae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ced495e8d49940ceb7ce900c229c7d04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac2538e758fa4b58930ae563cbb18db1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "63e2b2ab3b8c48e68dfeaec8b5f4128d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "020e0c9d87e54f74b81c1f62b3721802": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}